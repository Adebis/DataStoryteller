         Title Q-learning
           URL http://link.springer.com/article/10.1007/BF00992698
          Year 1992
     Citations 6916
      Versions 36
    Cluster ID 17289114859322381655
Citations list http://scholar.google.com/scholar?cites=17289114859322381655&as_sdt=2005&sciodt=0,5&hl=en
 Versions list http://scholar.google.com/scholar?cluster=17289114859322381655&hl=en&as_sdt=0,5&as_ylo=1992&as_yhi=1992
       Excerpt Abstract Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively 

         Title Practical issues in temporal difference learning
           URL http://scholar.google.com/https://papers.nips.cc/paper/465-practical-issues-in-temporal-difference-learning.pdf
          Year 1992
     Citations 1110
      Versions 28
    Cluster ID 9845693971640897794
      PDF link http://scholar.google.com/https://papers.nips.cc/paper/465-practical-issues-in-temporal-difference-learning.pdf
Citations list http://scholar.google.com/scholar?cites=9845693971640897794&as_sdt=2005&sciodt=0,5&hl=en
 Versions list http://scholar.google.com/scholar?cluster=9845693971640897794&hl=en&as_sdt=0,5&as_ylo=1992&as_yhi=1992
       Excerpt Abstract This paper examines whether temporal difference methods for training connectionist networks, such as Suttons's TO ('\) algorithm, can be successfully applied to complex real-world problems. A number of important practical issues are identified and 

         Title Self-improving reactive agents based on reinforcement learning, planning and teaching
           URL http://link.springer.com/article/10.1023/A:1022628806385
          Year 1992
     Citations 838
      Versions 16
    Cluster ID 7562600501657540815
Citations list http://scholar.google.com/scholar?cites=7562600501657540815&as_sdt=2005&sciodt=0,5&hl=en
 Versions list http://scholar.google.com/scholar?cluster=7562600501657540815&hl=en&as_sdt=0,5&as_ylo=1992&as_yhi=1992
       Excerpt Abstract To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement 

         Title On the handling of continuous-valued attributes in decision tree generation
           URL http://link.springer.com/article/10.1007/BF00994007
          Year 1992
     Citations 912
      Versions 21
    Cluster ID 15401162872632946603
Citations list http://scholar.google.com/scholar?cites=15401162872632946603&as_sdt=2005&sciodt=0,5&hl=en
 Versions list http://scholar.google.com/scholar?cluster=15401162872632946603&hl=en&as_sdt=0,5&as_ylo=1992&as_yhi=1992
       Excerpt Abstract We present a result applicable to classification learning algorithms that generate decision trees or rules using the information entropy minimization heuristic for discretizing continuous-valued attributes. The result serves to give a better understanding of the entropy 

         Title Introduction: The challenge of reinforcement learning
           URL http://www.springerlink.com/index/Q713133243750262.pdf
          Year 1992
     Citations 157
      Versions 6
    Cluster ID 3219563779377592670
      PDF link http://www.springerlink.com/index/Q713133243750262.pdf
Citations list http://scholar.google.com/scholar?cites=3219563779377592670&as_sdt=2005&sciodt=0,5&hl=en
 Versions list http://scholar.google.com/scholar?cluster=3219563779377592670&hl=en&as_sdt=0,5&as_ylo=1992&as_yhi=1992
       Excerpt Reinforcement learning is the learning of a mapping from situations to actions so as to maximize a scalar reward or reinforcement signal. The learner is not told which action to take, as in most forms of machine learning, but instead must discover which actions yield the 

         Title Transfer of learning by composing solutions of elemental sequential tasks
           URL http://link.springer.com/article/10.1023/A:1022680823223
          Year 1992
     Citations 377
      Versions 19
    Cluster ID 10750110895768415137
Citations list http://scholar.google.com/scholar?cites=10750110895768415137&as_sdt=2005&sciodt=0,5&hl=en
 Versions list http://scholar.google.com/scholar?cluster=10750110895768415137&hl=en&as_sdt=0,5&as_ylo=1992&as_yhi=1992
       Excerpt Abstract Although building sophisticated learning agents that operate in complex environments will require learning to perform multiple tasks, most applications of reinforcement learning have focused on single tasks. In this paper I consider a class of 

         Title Simple statistical gradient-following algorithms for connectionist reinforcement learning
           URL http://link.springer.com/article/10.1007/BF00992696
          Year 1992
     Citations 1495
      Versions 22
    Cluster ID 11418084539727663855
Citations list http://scholar.google.com/scholar?cites=11418084539727663855&as_sdt=2005&sciodt=0,5&hl=en
 Versions list http://scholar.google.com/scholar?cluster=11418084539727663855&hl=en&as_sdt=0,5&as_ylo=1992&as_yhi=1992
       Excerpt Abstract This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along 

         Title A further comparison of splitting rules for decision-tree induction
           URL http://www.springerlink.com/index/U667655325380233.pdf
          Year 1992
     Citations 351
      Versions 9
    Cluster ID 15939375025409647416
      PDF link http://www.springerlink.com/index/U667655325380233.pdf
Citations list http://scholar.google.com/scholar?cites=15939375025409647416&as_sdt=2005&sciodt=0,5&hl=en
 Versions list http://scholar.google.com/scholar?cluster=15939375025409647416&hl=en&as_sdt=0,5&as_ylo=1992&as_yhi=1992
       Excerpt One approach to learning classification rules from examples is to build decision trees. A review and comparison paper by Mingers (Mingers, 1989) looked at the first stage of tree building, which uses a “splitting rule” to grow trees with a greedy recursive partitioning 

         Title The utility of knowledge in inductive learning
           URL http://www.springerlink.com/index/RVM82J2528U51583.pdf
          Year 1992
     Citations 447
      Versions 17
    Cluster ID 4266860396616693331
      PDF link http://www.springerlink.com/index/RVM82J2528U51583.pdf
Citations list http://scholar.google.com/scholar?cites=4266860396616693331&as_sdt=2005&sciodt=0,5&hl=en
 Versions list http://scholar.google.com/scholar?cluster=4266860396616693331&hl=en&as_sdt=0,5&as_ylo=1992&as_yhi=1992
       Excerpt In this paper, we demonstrate how different forms of background knowledge can be integrated with an inductive method for generating function-free Horn clause rules. Furthermore, we evaluate, both theoretically and empirically, the effect that these forms of 

         Title Learning conjunctions of Horn clauses
           URL http://link.springer.com/article/10.1023/A:1022689015665
          Year 1992
     Citations 268
      Versions 15
    Cluster ID 14547438351581097021
Citations list http://scholar.google.com/scholar?cites=14547438351581097021&as_sdt=2005&sciodt=0,5&hl=en
 Versions list http://scholar.google.com/scholar?cluster=14547438351581097021&hl=en&as_sdt=0,5&as_ylo=1992&as_yhi=1992
       Excerpt Abstract An algorithm is presented for learning the class of Boolean formulas that are expressible as conjunctions of Horn clauses.(A Horn clause is a disjunction of literals, all but at most one of which is a negated variable.) The algorithm uses equivalence queries and 

